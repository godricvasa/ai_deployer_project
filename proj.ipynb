{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(\n",
    "    model=\"gpt-4o-mini\",\n",
    "   # api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish_reason='stop' content='- **Entrepreneurial Ventures:** Elon Musk is known for founding and leading several high-profile companies, including Tesla, Inc., which revolutionized the electric vehicle market, and SpaceX, which aims to reduce space transportation costs and enable Mars colonization.\\n\\n- **Innovative Technologies:** Musk is a proponent of advanced technologies such as Neuralink, which focuses on brain-machine interfaces, and The Boring Company, which targets urban transportation challenges through tunneling solutions.' usage=RequestUsage(prompt_tokens=18, completion_tokens=93) cached=False logprobs=None thought=None\n"
     ]
    }
   ],
   "source": [
    "from autogen_core.models import UserMessage\n",
    "\n",
    "result = await model_client.create(\n",
    "    [UserMessage(content=\"Tell me about Elon Musk in 2 bullet points\", source=\"user\")]\n",
    "    )\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flask\n",
      "├── README.md\n",
      "├── app\n",
      "│   ├── Dockerfile\n",
      "│   ├── app.py\n",
      "│   └── requirements.txt\n",
      "└── compose.yaml\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_project_tree_string(project_path, max_depth=3, indent=\"    \"):\n",
    "    \"\"\"\n",
    "    Generates a tree-like string representation of a project directory up to a given depth.\n",
    "\n",
    "    Args:\n",
    "        project_path (str): The path to the project directory.\n",
    "        max_depth (int): Maximum depth to traverse.\n",
    "        indent (str): Indentation used for each level.\n",
    "\n",
    "    Returns:\n",
    "        str: A tree-like formatted string of the project structure.\n",
    "    \"\"\"\n",
    "    def traverse(directory, depth=0, prefix=\"\"):\n",
    "        if depth > max_depth:\n",
    "            return \"\"  # Stop recursion at max depth\n",
    "        \n",
    "        tree_str = \"\"\n",
    "        try:\n",
    "            items = sorted(os.listdir(directory))  # Sort to maintain order\n",
    "            for i, item in enumerate(items):\n",
    "                item_path = os.path.join(directory, item)\n",
    "                is_last = (i == len(items) - 1)\n",
    "                branch = \"└── \" if is_last else \"├── \"\n",
    "\n",
    "                tree_str += f\"{prefix}{branch}{item}\\n\"\n",
    "\n",
    "                if os.path.isdir(item_path):\n",
    "                    extension = \"    \" if is_last else \"│   \"\n",
    "                    tree_str += traverse(item_path, depth + 1, prefix + extension)\n",
    "        except PermissionError:\n",
    "            tree_str += f\"{prefix}└── [Permission Denied]\\n\"\n",
    "        \n",
    "        return tree_str\n",
    "\n",
    "    return f\"{os.path.basename(project_path)}\\n\" + traverse(project_path)\n",
    "\n",
    "# Example usage\n",
    "project_path = \"C:\\\\Users\\\\vasanth.mu\\\\Desktop\\\\genai-training\\\\fin-5\\\\awesome-compose\\\\flask\"\n",
    "project_tree = get_project_tree_string(project_path)\n",
    "print(project_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- assistant ----------\n",
      "[FunctionCall(id='call_dXvf76DofxAy7bEtwqNjXptB', arguments='{\"query\":\"import os\\\\nos.listdir(\\'C:\\\\\\\\Users\\\\\\\\vasanth.mu\\\\\\\\Desktop\\\\\\\\genai-training\\\\\\\\fin-5\\\\\\\\awesome-compose\\\\\\\\flask\\')\"}', name='python_repl_ast')]\n",
      "---------- assistant ----------\n",
      "[FunctionExecutionResult(content=\"SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\\\UXXXXXXXX escape (<unknown>, line 2)\", name='python_repl_ast', call_id='call_dXvf76DofxAy7bEtwqNjXptB', is_error=False)]\n",
      "---------- assistant ----------\n",
      "SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (<unknown>, line 2)\n"
     ]
    }
   ],
   "source": [
    "#make the agent for docker file generation using the project strucure using python repl tool \n",
    "import pandas as pd\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "tool = LangChainToolAdapter(PythonAstREPLTool(locals={\"project_path\": project_path}))\n",
    "\n",
    "# AI Model\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Define AI Assistant Agent\n",
    "agent = AssistantAgent(\n",
    "    \"assistant\",\n",
    "    tools=[tool],\n",
    "    model_client=model_client,\n",
    "    system_message=(\n",
    "        \"You are an AI DevOps assistant. \"\n",
    "        \"structure.use the python repl tool for getting the tree structure for the give project and then \"\n",
    "        \"Your task is to generate a `Dockerfile` for the given project.. You can access the project path from the project_path variable\"\n",
    "        \"\\n\\nProject path:\\n\" + project_path\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run AI Query\n",
    "res = await Console(\n",
    "    agent.on_messages_stream(\n",
    "        [TextMessage(content=\"Generate a Dockerfile for this project.\", source=\"user\")], CancellationToken()\n",
    "    ),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasanth.mu\\AppData\\Local\\Temp\\ipykernel_25904\\577462038.py:15: UserWarning: The current event loop policy is not WindowsProactorEventLoopPolicy. This may cause issues with subprocesses. Try setting the event loop policy to WindowsProactorEventLoopPolicy. For example: `asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())`. See https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop.\n",
      "  local_executor = LangChainToolAdapter(LocalCommandLineCodeExecutor(work_dir=project_path))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LocalCommandLineCodeExecutor' object has no attribute 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m project_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvasanth.mu\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mgenai-training\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfin-5\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mawesome-compose\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mflask\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize Local Command Line Executor (Executes shell commands in the given directory)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m local_executor \u001b[38;5;241m=\u001b[39m \u001b[43mLangChainToolAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLocalCommandLineCodeExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwork_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m agent \u001b[38;5;241m=\u001b[39m AssistantAgent(\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     tools\u001b[38;5;241m=\u001b[39m[local_executor],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     ),\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Run AI Query\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vasanth.mu\\Desktop\\genai-training\\.venv\\Lib\\site-packages\\autogen_ext\\tools\\langchain\\_langchain_adapter.py:149\u001b[0m, in \u001b[0;36mLangChainToolAdapter.__init__\u001b[1;34m(self, langchain_tool)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_langchain_tool: LangChainTool \u001b[38;5;241m=\u001b[39m langchain_tool\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Extract name and description\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_langchain_tool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[0;32m    150\u001b[0m description \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_langchain_tool\u001b[38;5;241m.\u001b[39mdescription \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# Determine the callable method\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LocalCommandLineCodeExecutor' object has no attribute 'name'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n",
    "\n",
    "# Define project path\n",
    "# Define project path\n",
    "project_path = r\"C:\\Users\\vasanth.mu\\Desktop\\genai-training\\fin-5\\awesome-compose\\flask\"\n",
    "\n",
    "# Initialize Local Command Line Executor (Executes shell commands in the given directory)\n",
    "local_executor = LangChainToolAdapter(LocalCommandLineCodeExecutor(work_dir=project_path))\n",
    "\n",
    "agent = AssistantAgent(\n",
    "    \"assistant\",\n",
    "    tools=[local_executor],\n",
    "    model_client=model_client,\n",
    "    system_message=(\n",
    "        \"You are an AI DevOps assistant with expertise in containerization and project structuring. \"\n",
    "        \"Follow these steps strictly:\\n\"\n",
    "        \"1. **Recursively list all files** in the project directory and its subdirectories.\\n\"\n",
    "        \"2. **Identify the tech stack** based on specific files:\\n\"\n",
    "        \"   - `requirements.txt` → Flask/Python (use `pip install` for dependencies)\\n\"\n",
    "        \"   - `package.json` → Node.js (use `npm install`)\\n\"\n",
    "        \"   - `pom.xml` → Java (use Maven `mvn install`)\\n\"\n",
    "        \"3. **Generate a fully optimized `Dockerfile`** based on the detected stack.\\n\"\n",
    "        \"4. **Ensure best practices**, including:\\n\"\n",
    "        \"   - Using lightweight base images (`python:3.9-slim`, `node:18-alpine`, etc.)\\n\"\n",
    "        \"   - Installing only necessary dependencies\\n\"\n",
    "        \"   - Setting appropriate working directories and entry points\\n\"\n",
    "        \"5. **Return the complete `Dockerfile` as the final response.**\\n\\n\"\n",
    "        \"Project Structure Example:\\n\"\n",
    "        \"```\\n\"\n",
    "        \"flask/\\n\"\n",
    "        \"├── README.md\\n\"\n",
    "        \"├── app/\\n\"\n",
    "        \"│   ├── Dockerfile\\n\"\n",
    "        \"│   ├── app.py\\n\"\n",
    "        \"│   └── requirements.txt\\n\"\n",
    "        \"└── compose.yaml\\n\"\n",
    "        \"```\\n\\n\"\n",
    "        \"Project Path:\\n\" + project_path\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Run AI Query\n",
    "res = await Console(\n",
    "    agent.on_messages_stream(\n",
    "        [TextMessage(content=\"List all files and generate a Dockerfile based on the project structure.\", source=\"user\")],\n",
    "        cancellation_token=CancellationToken(),\n",
    "    )\n",
    ")\n",
    "\n",
    "print(res.chat_message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autogen_agentchat.llm_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Configure your model client (OpenAI, Azure, etc.)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mautogen_agentchat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatCompletionConfig\n\u001b[0;32m     15\u001b[0m model_config \u001b[38;5;241m=\u001b[39m ChatCompletionConfig(\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Replace with your model\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Use your API key from environment variables\u001b[39;00m\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Define project path - the path you want to containerize\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'autogen_agentchat.llm_config'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "import json\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor\n",
    "import asyncio\n",
    "\n",
    "# Configure your model client (OpenAI, Azure, etc.)\n",
    "from autogen_agentchat.llm_config import ChatCompletionConfig\n",
    "model_config = ChatCompletionConfig(\n",
    "    model=\"gpt-4\",  # Replace with your model\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\")  # Use your API key from environment variables\n",
    ")\n",
    "\n",
    "# Define project path - the path you want to containerize\n",
    "project_path = r\"C:\\Users\\vasanth.mu\\Desktop\\genai-training\\fin-5\\awesome-compose\\flask\"\n",
    "\n",
    "# Create two local executors - one for examining files and one for Docker operations\n",
    "file_explorer = LangChainToolAdapter(\n",
    "    LocalCommandLineCodeExecutor(work_dir=project_path)\n",
    ")\n",
    "\n",
    "docker_executor = LangChainToolAdapter(\n",
    "    LocalCommandLineCodeExecutor(work_dir=project_path)\n",
    ")\n",
    "\n",
    "# Create a Python REPL tool for more complex operations\n",
    "python_repl = LangChainToolAdapter(PythonAstREPLTool())\n",
    "\n",
    "# Define the agent workflow in separate functions for clarity\n",
    "async def run_docker_containerization_workflow():\n",
    "    \"\"\"Main function to run the containerization workflow\"\"\"\n",
    "    \n",
    "    # Create the assistant agent with appropriate tools and system message\n",
    "    assistant = AssistantAgent(\n",
    "        \"DockerBuilder\",\n",
    "        tools=[file_explorer, docker_executor, python_repl],\n",
    "        model_client=model_config,\n",
    "        system_message=(\n",
    "            \"You are an AI DevOps specialist with expertise in containerization. \"\n",
    "            \"Follow these steps in order:\\n\\n\"\n",
    "            \n",
    "            \"1. **ANALYZE PROJECT STRUCTURE**:\\n\"\n",
    "            \"   - First, list all files in the project to understand its structure\\n\"\n",
    "            \"   - Use shell commands like `dir /s /b` (Windows) or `find . -type f | sort` (Linux/Mac)\\n\\n\"\n",
    "            \n",
    "            \"2. **IDENTIFY TECH STACK AND DEPENDENCIES**:\\n\"\n",
    "            \"   - Look for key files that identify the tech stack:\\n\"\n",
    "            \"     * `requirements.txt` → Python/Flask (examine contents for specific packages)\\n\"\n",
    "            \"     * `package.json` → Node.js (check dependencies section)\\n\"\n",
    "            \"     * `pom.xml` or `build.gradle` → Java\\n\"\n",
    "            \"     * `go.mod` → Go\\n\"\n",
    "            \"     * `composer.json` → PHP\\n\"\n",
    "            \"   - Check for framework-specific files: app.py (Flask), index.js (Node), etc.\\n\"\n",
    "            \"   - Examine configuration files (e.g., .env, config.yaml) for required environment variables\\n\\n\"\n",
    "            \n",
    "            \"3. **DETERMINE PORTS AND ENVIRONMENT REQUIREMENTS**:\\n\"\n",
    "            \"   - Look for port configurations in main application files\\n\"\n",
    "            \"   - Check for environment variables used by the application\\n\\n\"\n",
    "            \n",
    "            \"4. **CREATE OPTIMIZED DOCKERFILE**:\\n\"\n",
    "            \"   - Choose the most appropriate lightweight base image for the tech stack\\n\"\n",
    "            \"   - Include only necessary dependencies and build tools\\n\"\n",
    "            \"   - Follow Dockerfile best practices (multi-stage builds when appropriate)\\n\"\n",
    "            \"   - Set proper working directories, users, and permissions\\n\"\n",
    "            \"   - Configure appropriate EXPOSE directives for ports\\n\"\n",
    "            \"   - Add health checks if possible\\n\\n\"\n",
    "            \n",
    "            \"5. **VERIFY AND TEST**:\\n\"\n",
    "            \"   - Validate the Dockerfile syntax\\n\"\n",
    "            \"   - Suggest commands to build and test the Docker image\\n\\n\"\n",
    "            \n",
    "            \"Project Path: \" + project_path\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # Create a user proxy with appropriate configuration\n",
    "    user = UserProxyAgent(\n",
    "        \"User\",\n",
    "        human_input_mode=\"NEVER\",  # No human input needed for automation\n",
    "        max_consecutive_auto_reply=10,\n",
    "        default_auto_reply=\"Continue the analysis.\",\n",
    "    )\n",
    "    \n",
    "    # Run the conversation\n",
    "    initial_message = TextMessage(\n",
    "        content=(\n",
    "            \"Analyze the project at the specified path and create an optimized Dockerfile. \"\n",
    "            \"Start by exploring the project structure, then identify the tech stack, and finally \"\n",
    "            \"create a production-ready Dockerfile with best practices. Be systematic in your approach.\"\n",
    "        ),\n",
    "        source=\"User\"\n",
    "    )\n",
    "    \n",
    "    # Use Console to show the conversation stream\n",
    "    result = await Console(\n",
    "        assistant.on_messages_stream(\n",
    "            [initial_message],\n",
    "            cancellation_token=CancellationToken(),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Return the final result\n",
    "    return result.chat_message.content\n",
    "\n",
    "# Function to initialize and run the workflow\n",
    "async def main():\n",
    "    result = await run_docker_containerization_workflow()\n",
    "    print(\"\\n=== FINAL RESULT ===\\n\")\n",
    "    print(result)\n",
    "\n",
    "# Run the workflow\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- assistant ----------\n",
      "[FunctionCall(id='call_539ArTuuJqoyY5v3b2EmOM2Y', arguments='{\"query\":\"# Creating a Dockerfile for a Flask application\\\\n\\\\n# A Dockerfile typically contains the following instructions:\\\\n\\\\ndockerfile_content = \\'\\'\\'\\\\n# Use the official Python base image\\\\nFROM python:3.9-slim\\\\n\\\\n# Set the working directory\\\\nWORKDIR /app\\\\n\\\\n# Copy the requirements file\\\\nCOPY app/requirements.txt .\\\\n\\\\n# Install dependencies\\\\nRUN pip install --no-cache-dir -r requirements.txt\\\\n\\\\n# Copy the rest of the application code\\\\nCOPY app/ .\\\\n\\\\n# Expose the application\\'s port\\\\nEXPOSE 5000\\\\n\\\\n# Define the command to run the application\\\\nCMD [ \\\\\"python\\\\\", \\\\\"app.py\\\\\" ]\\\\n\\'\\'\\' \\\\n\\\\ndockerfile_content\"}', name='python_repl_ast')]\n",
      "---------- assistant ----------\n",
      "[FunctionExecutionResult(content='\\n# Use the official Python base image\\nFROM python:3.9-slim\\n\\n# Set the working directory\\nWORKDIR /app\\n\\n# Copy the requirements file\\nCOPY app/requirements.txt .\\n\\n# Install dependencies\\nRUN pip install --no-cache-dir -r requirements.txt\\n\\n# Copy the rest of the application code\\nCOPY app/ .\\n\\n# Expose the application\\'s port\\nEXPOSE 5000\\n\\n# Define the command to run the application\\nCMD [ \"python\", \"app.py\" ]\\n', name='python_repl_ast', call_id='call_539ArTuuJqoyY5v3b2EmOM2Y', is_error=False)]\n",
      "---------- assistant ----------\n",
      "\n",
      "# Use the official Python base image\n",
      "FROM python:3.9-slim\n",
      "\n",
      "# Set the working directory\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy the requirements file\n",
      "COPY app/requirements.txt .\n",
      "\n",
      "# Install dependencies\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "# Copy the rest of the application code\n",
      "COPY app/ .\n",
      "\n",
      "# Expose the application's port\n",
      "EXPOSE 5000\n",
      "\n",
      "# Define the command to run the application\n",
      "CMD [ \"python\", \"app.py\" ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#make the agent for docker file generation using the project strucure using python repl tool \n",
    "import pandas as pd\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "tool = LangChainToolAdapter(PythonAstREPLTool(locals={\"project_structure\": project_tree}))\n",
    "\n",
    "# AI Model\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Define AI Assistant Agent\n",
    "agent = AssistantAgent(\n",
    "    \"assistant\",\n",
    "    tools=[tool],\n",
    "    model_client=model_client,\n",
    "    system_message=(\n",
    "        \"You are an AI DevOps assistant. Your task is to generate a `Dockerfile` for the given project \"\n",
    "        \"structure.\"\n",
    "        \"\\n\\nProject Structure:\\n\" + project_tree\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run AI Query\n",
    "res = await Console(\n",
    "    agent.on_messages_stream(\n",
    "        [TextMessage(content=\"Generate a Dockerfile for this project.\", source=\"user\")], CancellationToken()\n",
    "    ),\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Use the official Python base image\\nFROM python:3.9-slim\\n\\n# Set the working directory\\nWORKDIR /app\\n\\n# Copy the requirements file\\nCOPY app/requirements.txt .\\n\\n# Install dependencies\\nRUN pip install --no-cache-dir -r requirements.txt\\n\\n# Copy the rest of the application code\\nCOPY app/ .\\n\\n# Expose the application\\'s port\\nEXPOSE 5000\\n\\n# Define the command to run the application\\nCMD [ \"python\", \"app.py\" ]\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.chat_message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dockerfile: '\n",
      "\n",
      "  # Use the official Python base image\n",
      "\n",
      "  FROM python:3.9-slim\n",
      "\n",
      "\n",
      "  # Set the working directory\n",
      "\n",
      "  WORKDIR /app\n",
      "\n",
      "\n",
      "  # Copy the requirements file\n",
      "\n",
      "  COPY app/requirements.txt .\n",
      "\n",
      "\n",
      "  # Install dependencies\n",
      "\n",
      "  RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "\n",
      "  # Copy the rest of the application code\n",
      "\n",
      "  COPY app/ .\n",
      "\n",
      "\n",
      "  # Expose the application''s port\n",
      "\n",
      "  EXPOSE 5000\n",
      "\n",
      "\n",
      "  # Define the command to run the application\n",
      "\n",
      "  CMD [ \"python\", \"app.py\" ]\n",
      "\n",
      "  '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "def dockerfile_to_yaml(dockerfile_content):\n",
    "    \"\"\"\n",
    "    Converts a Dockerfile string into a YAML-formatted multiline string.\n",
    "    \n",
    "    Args:\n",
    "        dockerfile_content (str): The raw Dockerfile content.\n",
    "\n",
    "    Returns:\n",
    "        str: YAML string with Dockerfile content properly formatted.\n",
    "    \"\"\"\n",
    "    yaml_data = {\"dockerfile\": dockerfile_content}  # Store Dockerfile under the \"dockerfile\" key\n",
    "    return yaml.dump(yaml_data, default_style=None, default_flow_style=False)\n",
    "\n",
    "# Example Dockerfile content (string)\n",
    "\n",
    "\n",
    "# Convert to YAML\n",
    "yaml_output = dockerfile_to_yaml(res.chat_message.content)\n",
    "\n",
    "# Print YAML output\n",
    "print(yaml_output)\n",
    "\n",
    "# Optional: Save to a YAML file\n",
    "with open(\"dockerfile.yaml\", \"w\") as file:\n",
    "    file.write(yaml_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
